\section{Microarray technology}
\label{sec:microarray-methods}

Microarray technology relies on the non-covalent, sequence-specific interaction
between complementary strands of nucleic acids\cite{watson_molecular_1953} to
detect and quantify specific populations of m\smallcaps{rna} in a solution.  A
microarray chip consists of a universe of oligonucleotide probes attached to a
substrate through covalent bonds.  Each such probe is synthesized to
specifically match a unique messenger \smallcaps{rna} molecule.  When the chip
is exposed to a solution of fluorecently labeled m\smallcaps{rna}s, only those
that hybridize with their respective probes will be retained upon washing off
non-specific bonding sequences.  This allows for the quantification of the
fluorescent signals emitted when the chip is scanned with a laser beam of a
specific wavelength.  The measured signals relay the relative quantity of each
m\smallcaps{rna} molecule assayed by the microarray, as each spot has a known
position on the chip (Figure~\ref{fig:microarray-economist}).

% Oligonucleotide microarrays often carry control probes designed to hybridize
% with RNA spike-ins. The degree of hybridization between the spike-ins and the
% control probes is used to normalize the hybridization measurements for the
% target probes.

\begin{marginfigure}%
  \begin{center}
    % \includegraphics[width=9cm]{microarrays-economist.jpg}
    \includegraphics{microarrays-economist-17Feb2015.pdf}
    \caption[Schematic representation of how microarrays work]{A schematic
      representation of how microarrays work.  \textbf{1.}~Microarrays rely on a
      fundamental property of nucleic acids, the monomeric units that polymerize
      into \smallcaps{dna} or \smallcaps{rna} strands.  Adenine (\smallcaps{A})
      are complementary to thymine (\smallcaps{t}), and cytosine (\smallcaps{c})
      are complementary to guanine (\smallcaps{g}).  Just one incorrect base can
      prevent two strands from binding.  \textbf{2.}~A microarray typically
      contains thousands of squares, or spots.  Each spot anchors many copies of
      a particular sequence of single-stranded \smallcaps{dna}, corresponding to
      a particular gene.  \textbf{3.}~Messenger \smallcaps{rna} fragments
      extracted from a tissue and labeled with different fluorescent dyes are
      washed over the microarray and hybridize with \smallcaps{dna} strands with
      the complementary sequence.  \textbf{4.}~The dyes are illuminated using
      fluorescent light.  It is then possible to show which \smallcaps{rna}
      fragments were retained in which spots---and hence which genes were being
      expressed in the tissue from which the \smallcaps{rna} was extracted.
      Source: \emph{The Economist;
        Affymetrix}.}\label{fig:microarray-economist}%
  \end{center}
\end{marginfigure}

\section{Microarray data preprocessing}
\label{sec:microarray-methods-data-preprocessing}

By virtue of their design, microarrays allow for the monitoring of expression
levels for thousands of gene transcription products simultaneously.  Microarray
expression data are thus characterized by high dimensionality and noisiness.
This prompts the need for preprocessing methods aiming at removing systematic
biases in expression measurements, introduced during
experimentation.\cite{shakya_comparison_2010}

The goal of microarray data preprocessing is to convert raw imaging data into
meaningful biological data and to enable comparison of results obtained from
different arrays.  It comprises three steps: (\emph{a})~the transformation of
image data into intensity values; (\emph{b})~the assessment of array quality;
and (\emph{c})~the removing of technical biases (through background adjustment,
normalization and feature filtering and summarization).

The digital imaging of fluorescence signals is typically performed by
proprietary software designed by the microarray manufacturer.  These software
packages assign coordinates to each spot in the array, quantify signal intensity
and uniformity of each spot, and compare their signal intensity relative to
background.

Quality control is performed by visual inspection of imaging data from the
scanner or the platform software, with special attention given to washing
artifacts, odd or missing spots, and array uniformity.  Aberrant chips may have
to be discarded at this stage.

Chips meeting quality standards then undergo background adjustment and
normalization.  Normalization methods aim to compensate for procedural biases
that are independent from biological signal.  Early approaches for microarray
normalization were based on the assumption that most genes, and in particular
so-called housekeeping genes,\footnote{\emph{Housekeeping} genes are genes
  defined as participating in basic, thus universal, cellular processes.}
should have similar expression levels across samples.  Housekeeping genes have
since been shown to vary in expression by 30\% or more across healthy samples,
and even more in tumour samples.\cite{lee_control_2002,eisenberg_human_2003}
Data-driven normalization approaches were then developed, such as median
correction,\cite{cho_genome-wide_1998,selinger_rna_2000} variance stabilizing
transformation,\cite{durbin_variance-stabilizing_2002} locally weighted linear
regression\cite{yang_normalization_2002} and spline based
methods.\cite{workman_new_2002} Data-driven presupposes that most of the
observed variation in expression values is due to technical biases rather than
to biological sources.\cite{hicks_when_2014}

Normalization strategies for double-channel microarrays (spotted oligonucleotide
or c\smallcaps{dna} arrays\cite{schena_quantitative_1995}) are different from
those for single-channel microarrays (\emph{in situ} synthesized high density
oligonucleotide arrays,\cite{lockhart_expression_1996} such as \emph{Affymetrix
  GeneChip}).  \emph{Affymetrix GeneChip} arrays use multiple probes per gene
and a single-colour detection system, as one sample is hybridized per chip.
Spotted oligonucleotide or c\smallcaps{dna} arrays use one probe per gene and a
two-colour scheme, where two different samples are hybridized on the same array.
Consequently, single-channel arrays measure the overall abundance of a probe
sequence in a target sample, whereas c\smallcaps{dna} arrays measure the
relative abundance of a probe sequence in two target samples.
% In other words, the expression measures for single-channel arrays are absolute
% (log) intensities, whereas they represent (log) ratios of intensities for
% c\smallcaps{dna} arrays.\footnote{In many cases, one of the samples in a
% c\smallcaps{DNA} array hybridization is a common reference used across
% multiple slides and whose sole purpose is to provide a baseline for direct
% comparison of expression measures between arrays.}
As a result, normalization of single-channel microarrays is performed at the
``between-array'' level, whereas normalization of double-channel microarrays is
conducted at the ``within-array'' level.\cite{do_normalization_2006}

For double-channel arrays chips, normalization methods commonly seek to remove
biases within each array with local regression algorithms.  Terry Speed's lab,
at Berkeley, identified an intensity-dependent dye bias concerning
c\smallcaps{dna} arrays.  In these arrays, the $\log_2$ of the dye intensity
ratios shows a systematic dependence on intensity, characterized by a deviation
from zero for low-intensity spots.  Frequently, under-expressed genes appear
up-regulated in the red channel ($R$), and moderately expressed genes appear
up-regulated in the green channel ($G$).
% This artifact is the result of a ``quenching'' effect, whereby dye molecules
% in close proximity tend to re-absorb light from each other, hence diminishing
% the signal.  The amount of re-absorption is a function of the template
% concentration and differs for the two dyes.
This effect can be visualized by plotting the measured
$\log_2(\frac{R_{i}}{G_{i}})$ for each feature in the array as a function of the
$\log_2(R_{i}G_{i})$ product intensities.  This ratio-intensity plot is termed
\smallcaps{ma} plot.\footnote[][-4.5cm]{The name of the plot comes from
  ``minus'' and ``add'', respectively the ratio and product in the logarithmic
  scale.}  This technical bias may be corrected by fitting a locally weighted
regression, known as \emph{lowess} smoothing
(Figure~\ref{fig:ma-plot}).\cite[-4.2cm]{yang_normalization_2001} More specific
sources of technical bias, including spatially-dependent bias resulting from the
print tips used in the manufacturing process of the array, may also be addressed
by a \emph{lowess}-based, within group normalization.

\begin{marginfigure}[-4.2cm]%
  \begin{center}
    \includegraphics{lowess-normalization-23Feb2013.pdf}
    \caption[\emph{Lowess} normalization]{Example of \emph{lowess}
      normalization.  \textbf{A:}~\smallcaps{ma} plot showing colour dye
      dependent bias.  \textbf{B:}~\smallcaps{ma} plot after correction with
      \emph{lowess} normalization
      (\citealp{yang_normalization_2002}).}\label{fig:ma-plot}%
  \end{center}
\end{marginfigure}

\emph{Affymetrix GeneChip} are the reference arrays in the single-channel class,
and the platform of choice for the development of between-array normalization
methods.  \emph{Affymetrix} chips consist of several tens of thousands
probe-sets.  A probe-set is a collection of probe pairs designed to interrogate
a specific sequence and contains \numrange{11}{20} probe pairs of 25-mer
oligonucleotides each.  Each probe pair consists of a perfect match probe
(\smallcaps{pm}) and a mismatch probe (\smallcaps{mm}).  The \smallcaps{mm}
probe differs from the \smallcaps{pm} probe by a single substitution at the
center base position, conceived to disturb the binding of the target gene
transcript.  This design allows for the quantification of background and
nonspecific hybridization effects.

Different between-array normalization methods have been proposed in the
literature.  The \smallcaps{mas5} algorithm\cite{hubbell_robust_2002} normalizes
each array independently and sequentially and uses the value of \smallcaps{mm}
probes to compute summarized averages with linear scaling.  The \smallcaps{rma}
(robust multi-array average) algorithm\cite{irizarry_exploration_2003}
normalizes the value of each probe by quantile normalization\footnote{Quantile
  normalization is a global adjustment method that assumes the statistical
  distribution of expression values of each sample is the same.  Normalization
  is achieved by imposing the same distribution to all samples, using an average
  distribution as reference.  The average distribution is estimated from the
  average of each quantile across samples.} in multiple arrays, neglecting
information from \smallcaps{mm} probes.  The \smallcaps{gcrma}
algorithm\cite{wu_model-based_2004} applies the same normalization and
summarization methods as \smallcaps{rma}, but uses probe sequence information to
estimate and correct for probe affinity to non-specific binding.  More recently,
the f\smallcaps{rma} (frozen \smallcaps{rma}) algorithm\cite{mccall_frozen_2010}
leverages pre-computed estimates of probe-specific effects and variance from
public microarray databases to, in concert with the information from new arrays,
normalize and summarize the data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% http://www.people.vcu.edu/~mreimers/OGMDA/normalize.expression.html
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The first ``\mbox{across-array}'' normalization methods relied on the
% assumption that most genes, and in particular ``housekeeping''
% genes\footnote{\emph{Housekeeping} genes are genes recognized as participating
% in basic, thus universal, cellular processes.}, should not be expected to be
% differentially expressed across samples.  However, housekeeping genes have
% since been shown to vary in expression by 30\% or more across healthy samples,
% and even more in tumour samples.\cite{lee_control_2002} This lead to new
% approaches to identify stable patterns of expression around which to pin
% global levels of expression.  Yet, these approaches have been shown to suffer
% from significant levels of cross-hybridization between \mbox{25-mers} in
% \emph{Affymetrix} chips.

% Terry Speed's lab, at Berkeley, identified an important intensity-dependent
% dye bias regarding double channel microarrays, and introduced a popular method
% for adjusting it.  In these arrays, the $log_{2} (ratio)$ shows a systematic
% dependence on intensity, characterized by a deviation from zero for
% low-intensity spots.  Typically, under-expressed genes appear up-regulated in
% the red channel ($R$) and moderately expressed genes appear up-regulated in
% the green channel ($G$).  This artifact is the result of a ``quenching''
% effect, whereby dye molecules in close proximity tend to re-absorb light from
% each other, hence diminishing the signal.  The amount of re-absorption is a
% function of the template concentration and differs for the two dyes.  This
% effect can be visualized by plotting the measured
% $log_{2}(\frac{R_{i}}{G_{i}})$ for each feature in the array as a function of
% the $log_{2}(R_{i}G_{i})$ product intensities.  This ratio-intensity plot is
% termed \smallcaps{ma} plot.\footnote{The name of the plot comes from ``minus''
% and ``add'', or ratio and product in the logarithmic scale.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Among these are methods designed to normalize single chips, known as
% ``within-arrays'' methods, such as quantile normalization, or \smallcaps{mas5}
% for \emph{Affymetrix} chips.

Several reviews on these and other normalization methods were produced in the
specialized
literature,\cite{ploner_correlation_2005,bolstad_comparison_2003,harr_comparison_2006}
and a more detailed technical exposition of the computational implementation of
these algorithms can be found in Gentleman et
al.,\cite{gentleman_bioinformatics_2006} in the context of the
\href{http://www.bioconductor.org/}{\textsf{Bioconductor}} project.\footnote{The
  Bioconductor project is an open source and open development software project
  for the analysis and comprehension of genomic data
  (\citealp{gentleman_bioconductor:_2004}).  It is rooted in the open source
  statistical computing environment \textsf{R}.}

\medskip

The normalized microarray data for $p$ genes and $n$ biological samples are
denoted by $X_{n \times p}$ such that $x_{ij}$ represents the expression of gene
$j$ of sample $i$.

\section{Microarray data analysis}
\label{sec:microarray-methods-data-analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% http://discover.nci.nih.gov/microarrayAnalysis/Exploratory.Analysis.jsp
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In cancer research, analysis of genomic experiments performed with
\smallcaps{dna} microarray data may address a variety of tasks, including
finding gene associations with particular phenotypes, tumour class discovery or
tumour class prediction.  The work contributed in this thesis concerns:
(\emph{a}) the class prediction of tumour types based on gene expression
profiles; and (\emph{b}) the modeling of association of gene expression profiles
with the time until a particular outcome is observed (e.g., death of a patient
or relapse of disease), using survival analysis.  Both these problems are
examples of application of supervised learning techniques to the analysis of
microarray data.

In machine learning,\footnote{Machine learning is a branch of computational
  science whose purpose is to implement algorithms capable to infer models from
  training data.  Models derived from machine learning methods are then expected
  to make predictions or to inform decisions on testing data.} supervised
learning methods seek to infer a function from labeled training data.
Conversely, unsupervised learning methods aim to find intrinsic structure from
unlabeled data.\cite{webb_statistical_2003} Unsupervised learning is thus suited
to uncover coherent genomic signals from microarray data, whereas supervised
learning can be used to find associations between genomic signals and phenotypic
classes of samples.

Due to the high dimensionality of expression profiles, methods for
dimensionality reduction are required for microarray data analysis.  These
include \emph{feature transformation} methods and \emph{feature selection}
methods.\cite{haibe-kains_identification_2009}

Feature transformation is an unsupervised approach that consists in reducing the
feature space of a microarray gene expression matrix, such that new features
retain biological pertinence, maximum information and generalizability to
similar experiments:
\begin{equation}
  \label{eq:feature-transformation}
  X_{n \times p} \to X'_{n \times p'} : p \gg p'.
\end{equation}
Examples of microarray feature transformation techniques include principal
component analysis and clustering methods.

Feature selection techniques, on the other hand, are supervised approaches to
reduce data dimensionality.  In this work, particular attention will be given to
gene expression signatures, which configure an example of microarray feature
selection.  Gene expression signatures, or metagenes, are collections of genes
sharing a combined expression pattern associated with a given phenotype.  The
range of biological phenotypes confining the characterization of expression
signatures include the modulation of signaling pathways,\cite{itadani_can_2008}
the specification of tumour classes,\cite{ramaswamy_multiclass_2001} or the
definition of distinct clinical outcomes.\cite{vant_veer_gene_2002} In the last
two decades, a wealth of microarray studies on perturbed \emph{in vitro}
biological systems have generated an extensive number of gene signatures related
to various cellular mechanisms.\cite{chibon_cancer_2013} Public repositories,
like the Gene Ontology consortium (\smallcaps{go}),\cite{ashburner_gene_2000}
the Kyoto Encyclopedia of Genes and Genomes
(\smallcaps{kegg}),\cite{kanehisa_kegg:_2000}
\mbox{GeneSigDB},\cite{culhane_genesigdb:_2012} or the Molecular Signatures
Database (\mbox{MSigDB}),\cite{subramanian_gene_2005} have sought to curate and
articulate this volume of information in order to guide the interpretation of
genome-wide expression profiles.  Because biologically motivated gene signatures
can act as surrogate markers for the molecular processes they capture, they
provide entry points for hypothesis testing in public microarray data.

The use of a common vocabulary to refer to microarray features is thus essential
to this goal.  Given the wide range of microarray platforms on the market,
preprocessing routines often produce genomic expression data with feature
annotations that are disjoint, inconsistent, or conflicting.  Computational
strategies to interface curated annotation databases in order to update and
standardize feature nomenclatures are generally poorly discussed in the
literature.  A solid foundation for \emph{in silico} solutions to bridge the gap
between the knowledge of transcript sequence and the knowledge of transcript
function is provided in Gentleman et al.\cite{gentleman_bioinformatics_2006}

Accordingly, for all datasets used in this work (described in the
\hyperref[sec:methods-datasets]{\textsf{Microarray datasets}} section),
\href{http://www.bioconductor.org/}{\textsf{Bioconductor}} resources were used
to update feature annotations, and referents for \smallcaps{hugo} Gene
Nomenclature Committee (\smallcaps{hgnc}) gene
symbols\footnote{\href{http://www.genenames.org/}{http://www.genenames.org/}}
were universally retained as feature descriptors.  Microarray gene annotation
may also be used to perform dimension reduction of the expression feature space.
Hence, expression matrices where multiple features addressed the expression of
the same gene product were collapsed using a \texttt{maxSum}
routine.\cite{miller_strategies_2011}

All analyses described in the \hyperref[chap:results]{\textsf{Results}} chapter
were conducted in the \href{http://www.r-project.org/}{\textsf{R}} environment
for statistical computing,\cite{r_core_team_r:_2014} with extensive use of
computational resources from the
\href{http://www.bioconductor.org/}{\textsf{Bioconductor}}
project.\cite{gentleman_bioconductor:_2004}

% metagenes. how are they derived. correlations with gene expression. they can
% behave as surrogate markers of biological processes, states or responses and
% be used as classifiers.  Metagenes can also be also non-biologically
% motivated, and thus be defined randomly.

% a word about meta-data resources and bioconductor tools.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% abstract of chapter seven of the book
%% Bioinformatics and Computational Biology Solutions Using R and Bioconductor
%% Meta-data Resources and Tools in Bioconductor
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Closing the gap between knowledge of sequence and knowledge of function
% requires aggressive, integrative use of biological research databases of many
% different types.  For greatest effectiveness, analysis processes and
% interpretation of analytic results must be guided using relevant knowledge
% about the systems under investigation.  However, this knowledge is often
% widely scattered and encoded in a variety of formats.  In this section, we
% consider some of the different sources of biological information as well as
% the software tools that can be used to access these data and to integrate them
% into an analysis.  Bioconductor provides tools for creating, distributing, and
% accessing annotation resources in ways that have been found effective in work-
% flows for statistical analysis of microarray and other high-throughput assays.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\medskip

This section will proceed with a brief overview of tools for visualization of
genomic data, namely heatmaps and multidimensional scaling.  It will then
discuss the unsupervised learning tools used in this work, including principal
component analysis, clustering analysis and a summary on machine learning
algorithms.  Next, it will cover tools to evaluate the performance of
classifiers, including \smallcaps{roc} curves (for binary classifiers) and
\smallcaps{gsea} (an algorithm that uses microarray data to determine whether a
gene expression classifier is statistically associated with any of two
phenotypic classes of biological samples).  Finally, it will detail survival
analysis, the branch of supervised learning that seeks to explain the
relationship between a number of measured features (gene expression data) and
the time duration until the occurrence of a particular event (survival outcome).

\subsection{Visualization techniques}
\label{sec:methods-visualization}
% 02Mar2015

\subsection{Principal component analysis}
\label{sec:methods-pc}
% 03Mar2015

\subsection{Clustering analysis}
\label{sec:methods-clustering}
% 04Mar2015

\subsection{Machine learning analysis}
\label{sec:methods-machine-learning}
% 05Mar2015

\subsection{Receiver operating characteristic curves}
\label{sec:methods-roc}
% 06Mar2015

\subsection{Gene set enrichment analysis}
\label{sec:methods-gsea}
% 07Mar2015

\subsection{Survival analysis}
\label{sec:methods-survival-analysis}

Survival analysis is a collection of statistical procedures for data analysis
for which the outcome variable of interest is time until the event
occurs.\cite{kleinbaum_survival_1996} In follow-up studies of cancer patients,
survival analysis is used to model association of the expression of genomic
markers in cancer biopsies with the time until a given clinical outcome is
observed.

Clinical outcomes may include death of the patient (overall survival, or
\smallcaps{os}), death of the patient caused by the cancer (disease-specific
survival, or \smallcaps{dss}), the finding of new metastases in the patient
(distant metastasis free survival, or \smallcaps{dmfs}) or recurrence of the
cancer (disease-free survival, or \smallcaps{dfs}).  \emph{Survival time} refers
to the lapse of time since the beginning of the study up to the moment when an
event is observed, regardless of the clinical outcome considered.  Whenever the
information about an individual's survival time is incomplete, that observation
is said to be censored (Figure~\ref{fig:censorship}).  This may be due because
the event was not observed by the end of the study (in which case the follow-up
time considered for that patient is the entire duration of the study) or because
the patient quit or withdrew from the study before its end (in which case the
follow-up time considered for that patient is the time up to dropping out).

\begin{marginfigure}%
  \includegraphics{censorship-28Feb2015.pdf}
  \caption[Right-censored survival data]{A schematic representation of
    right-censored survival data.  Survival time is said to be
    \emph{right-}censored when the information regarding the right side of the
    follow-up period is incomplete.  Observed events are denoted by (\CIRCLE).
    Censored observations are denoted by (\Circle).  Notice that patient
    \smallcaps{b} is also censored, as no event had been observed by the end
    of the study (see text for details).}\label{fig:censorship}%
\end{marginfigure}

Follow-up studies are not amenable to ordinary regression models because the
time to event is typically not normally distributed and these models cannot
incorporate censoring data.  Instead, survival analysis uses two functions to
estimate survival time, the \emph{survival} function and the \emph{hazard}
function.

The survival function, $S(t)$, describes the probability of an event occurring
later than some specified time $t$:
\begin{equation}
  \label{eq:survival-function}
  S(t) = P"r(T > t),
\end{equation}
where $T$ is a random variable for a patient's survival time.

The hazard function, $h(t)$, describes the instantaneous potential per unit time
for the event to occur, given the patient has survived up to time $t$:
\begin{equation}
  \label{eq:hazard-function}
  h(t) = \lim_{\Delta t \to 0}\frac{P"r\{t \leqslant T \less t + \Delta t
    \mid T \geqslant t \}}{\Delta t}.
\end{equation}

% While the survival function is non-increasing i.e., $S(t)$ heads downwards as
% $t$ increases, (Figure~\ref{fig:survival-function}), the hazard function
% describes a failure rate conditional to the interval of time considered, and
% can therefore be modeled by any number of distributions.

Both functions are related to each other.  However, while the survival function
is non-increasing (Figure~\ref{fig:survival-function}), the hazard function may
be modeled by any number of distributions, as it describes a failure rate
conditional to the interval of time considered.  Building on these functions,
survival analysis stipulates a suite of parametric, non-parametric and
semi-parametric methods to make inferences over survival time.  These methods
can then be used to ascertain the relationship between a variable of interest
and the time to a clinical outcome.

\begin{marginfigure}%
  \includegraphics{survival-function-28Feb2015.pdf}
  \caption[Survival function]{The survival function, \emph{S}\,(\emph{t}),
    describes the likelihood that a patient will have a lifetime exceeding time
    \emph{t}.  \textbf{A:}~The theoretical distribution is non-increasing, and
    characterized by \mbox{\emph{S}\,(0) = 1} and \mbox{\emph{S}\,($\infty$) =
      0}.  \textbf{B:}~In practice, the estimated survival function,
    \emph{\^{S}}\,(\emph{t}), often takes a shape of a step function.  Because
    study periods are never infinite and there may be competing risks for
    failure, it is likely that not all patients will experience a clinical
    outcome by the end of the study.}\label{fig:survival-function}%
\end{marginfigure}

In the biomedical literature, the most recognizable non-parametric method to
estimate the survival function is the Kaplan-Meier
estimator.\cite{kaplan_nonparametric_1958} The Kaplan-Meier estimator is defined
as the probability of surviving in a given length of time while considering time
in many small intervals.\footnote{\citealp[pp. 365--93]{altman_practical_1990}}
It estimates the probability of occurrence of an event at time $t$ by
cumulatively multiplying prior probabilities of survival at preceding $t_i$
intervals.  The Kaplan-Meier, or product limit estimator, is thus formulated as:
\begin{equation}
  \label{eq:kaplan-meier}
\hat{S}(t)=\prod_{t_i<t}\frac{n_i-d_i}{n_i},
\end{equation}
where $n_i$ and $d_i$ are, respectively and for each prior time interval $t_i$,
the number of patients at risk (right-censored observations are removed), and
the number of patients experiencing an event.  The Kaplan-Meier estimator can be
used to obtain univariate descriptive statistics for survival data or to compare
the survival time for two or more groups of subjects.

The logrank test is a non-parametric hypothesis test to compare the survival
distribution of two
samples.\cite{mantel_evaluation_1966,peto_asymptotically_1972} It challenges the
null hypothesis that there is no difference between the populations in the
probability of an event at any time point.  To do so, it compares the estimates
of the hazard functions of the two groups at each observed event time.  The
logrank test is based on the same assumptions as the Kaplan-Meier survival
curve---namely, that censoring is unrelated to prognosis, the survival
probabilities are the same for subjects recruited early and late in the study,
and the events happened at the times specified.\cite{bland_logrank_2004}
Importantly, both the Kaplan-Meier estimator and the logrank test are able to
incorporate right-censored survival data.

When modeling the presence of covariates or explanatory variables to explain
survival time, fully parametric and semi-parametric approaches are available.
Parametric approaches, like the accelerated life class of models, assume that
the effect of covariates is proportional with respect to survival
time.\cite{kalbfleisch_statistical_2011} Under this model,
\begin{equation}
  \label{eq:aft}
  S_1(t) = S_0(t/\gamma).
\end{equation}
Effectively, this means that the probability that a member of group one will be
alive at time $t$ is exactly the same as the probability that a member of group
zero will be alive at time $t/\gamma$.  In parametric models, the estimation of
the covariates is conditional to the prior definition of the hazard
distributions in both groups.

% \footnote{For instance, for $\lambda = 2$, this would be half the age, so the
% probability that a member of group one would be alive at age 40 (or 60) would
% be the same as the probability that a member of group zero would be alive at
% age 20 (or 30). Thus, $\lambda$ can be interpreted as affecting the passage of
% time.  In this example, people in group zero age ``twice as fast''.}

Alternatively, the Cox proportional hazards family of models assumes that the
effects of the covariates is proportional with respect to the
hazard.\cite{cox_regression_1972} This assumption obviates the need of
specifying the underlying hazard functions---the model only seeks to fit the
regression parameters, hence being referred to has semi-parametric.  In its
simplest form, it can be formulated in such a way that the hazard at time $t$
for an individual with covariates $X$ is assumed to be:
\begin{equation}
  \label{eq:proportional-hazards}
  h(t \mid X)=h_0(t) \exp(\beta_1 X_{1}+\ldots+\beta_pX_{p}).
\end{equation}
This model formulates the hazard of individual $i$ at time $t$ as the product of
a baseline hazard function, $h_0(t)$, and of a linear function of a set of $p$
covariates, which is exponentiated.  Along with the specification of the model,
Sir David Cox developed a maximum partial likelihood method for estimation
of its covariates.\cite{cox_regression_1972} A logrank statistic can be
derived as the score test for the Cox proportional hazards model comparing two
groups.  The term Cox regression refers to the combination of both the model and
the estimation procedure.  It has become the tool of choice to estimate the
association of the expression of genomic metagenes with differential survival
times in cancer follow-up studies.

% An important advantage The modeling of the survival time by the Kaplan-Meier
% can also incorporate right-censored data

% Parametric methods assume that the underlying distribution of survival times
% follow certain known probability distributions.  For instance, the hazard
% function can be assumed to:
% \begin{itemize}
% \item be constant over time, e.g., when considering the likelihood of an healthy
%   individual to become ill;
% \item follow an increasing Weibull distribution, e.g., when considering the
%   likelihood of death for cancer patients not responding to treatment---as it is
%   expected to increase with time;
% \item follow an decreasing Weibull distribution, e.g., when considering the
%   likelihood of death for post-surgical cancer patients with fair prognosis---as
%   it is expected to decrease with time;
% \item follow a lognormal distribution, e.g., when considering the likelihood of
%   death for tuberculosis patients---as it first increases upon diagnosis, only
%   to decrease later.
% \end{itemize}
% Once the parametric function of choice is defined, model parameters
% are usually estimated using an appropriate modification of a maximum likelihood
% procedure.

% be constant over time (e.g., when considering the likelihood of an healthy
% individual of becoming ill); follow an increasing Weibull distribution (e.g.,
% when considering the likelihood of death for cancer patients not responding to
% treatment, as it is expected to increase with time); follow a decreasing
% Weibull distribution (e.g., when considering the likelihood of death for
% post-surgical cancer patients, as it is expected to decrease with time), or
% follow a lognormal distribution (e.g., when considering the likelihood of
% death for tuberculosis patients, as it first increases after diagnosis, then
% decreases).

% In follow-up studies of cancer patients, it is generally of interest to
% describe the relationship of the expression of a biologically motivated
% metagene to the time to event---possibly in the presence of clinical
% covariates, such as age, treatment, or tumour class.  This calls for models
% that describe the relationship between the predictor variable and survival
% time.  Survival analysis offers a range of parametric, non-parametric and
% semi-parametric approaches to model this relationship.

\medskip

When considering the association of genomic markers with survival time, the
dependent variable is composed of two parts: one consisting of the time to the
event (or lack thereof), encoded as a non-negative number; the other registering
the event status, encoded as a binary variable (routinely 1 if the event is
observed, and 0 if not).

Typically, the predictor is also a binary class, the result of discretizing the
patients in groups of good and bad prognosis as a function of the metagene
classifier.  This requires a method to stratify the cohort with an unsupervised
classification procedure.  Venet et al.\cite{venet_most_2011}\,investigated
three methods to this goal: (\emph{a})~the standard spliting of the cohort along
the two main clusters defined by hierarchical clustering of the metagene's
expression data;
% \footnote{Computed with the \texttt{hcluster} function of the \textsf{amap}
% \textsf{R} package, and implemented with average linkage and (1
% -- \emph{correlation}) between pairs of observations as the dissimilarity
% measure.}
(\emph{b})~spliting the cohort along the two main clusters defined
by applying the \emph{k}-means clustering algorithm to the metagene's expression
data;
% \footnote{Computed with the \textsf{R} function \texttt{kmeans}.}
and (\emph{c})~spliting the cohort along the median of the metagene's first
principal component.
% \footnote{Computed with the \textsf{R} function \texttt{prcomp}.}
They confirmed that the methods based on the first principal
component (henceforth referred to as \textsf{PC1} method) yielded significantly
higher hazard ratios and smaller \emph{p}-values.

In this work, we chose to model survival time as a function of a single
\emph{continuous} predictor, defined by the first principal component of the
metagene's expression (or by a single vector of gene expression) throughout the
entire cohort.  This departure from the standard approach has the disadvantage
of making it impossible to calculate a Kaplan-Meier estimator for the
model---therefore lacking a visual representation and making it harder to assess
the validity of the proportional hazards assumption.  However, it has the
advantage of using a predictor variable that faithfully mirrors the patterns of
expression captured by the metagene and of not requiring the artificial
stratification of the cohort in (balanced or not) groups of differential
prognosis.

Whenever applicable, association of metagene expression with outcome was thusly
computed by fitting a proportional hazards regression model between predictor
and dependent variables with the \texttt{coxph} function of the \textsf{R}
\textsf{survival} package.

% We have shown this approach to be more sensitive (data not shown).

%%% Local Variables:
%%% TeX-engine: xetex
%%% mode: latex
%%% TeX-master: "../../thesis"
%%% End:
